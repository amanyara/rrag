2024-07-16 11:28:48 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 6, in <module>
2024-07-16 11:28:48 | ERROR | stderr |     from rrag.argument.parser import get_infer_args
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/__init__.py", line 8, in <module>
2024-07-16 11:28:48 | ERROR | stderr |     from .parser import get_eval_args, get_infer_args, get_train_args
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 47, in <module>
2024-07-16 11:28:48 | ERROR | stderr |     check_dependencies()
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/utils/misc.py", line 75, in check_dependencies
2024-07-16 11:28:48 | ERROR | stderr |     require_version("transformers>=4.41.2", "To fix: pip install transformers>=4.41.2")
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 111, in require_version
2024-07-16 11:28:48 | ERROR | stderr |     _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
2024-07-16 11:28:48 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 44, in _compare_versions
2024-07-16 11:28:48 | ERROR | stderr |     raise ImportError(
2024-07-16 11:28:48 | ERROR | stderr | ImportError: transformers>=4.41.2 is required for a normal functioning of this module, but found transformers==4.40.1.
2024-07-16 11:28:48 | ERROR | stderr | To fix: pip install transformers>=4.41.2
2024-07-16 11:29:20 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 6, in <module>
2024-07-16 11:29:20 | ERROR | stderr |     from rrag.argument.parser import get_infer_args
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/__init__.py", line 8, in <module>
2024-07-16 11:29:20 | ERROR | stderr |     from .parser import get_eval_args, get_infer_args, get_train_args
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 47, in <module>
2024-07-16 11:29:20 | ERROR | stderr |     check_dependencies()
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/utils/misc.py", line 77, in check_dependencies
2024-07-16 11:29:20 | ERROR | stderr |     require_version("accelerate>=0.30.1", "To fix: pip install accelerate>=0.30.1")
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 111, in require_version
2024-07-16 11:29:20 | ERROR | stderr |     _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
2024-07-16 11:29:20 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 44, in _compare_versions
2024-07-16 11:29:20 | ERROR | stderr |     raise ImportError(
2024-07-16 11:29:20 | ERROR | stderr | ImportError: accelerate>=0.30.1 is required for a normal functioning of this module, but found accelerate==0.30.0.
2024-07-16 11:29:20 | ERROR | stderr | To fix: pip install accelerate>=0.30.1
2024-07-16 11:29:38 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 6, in <module>
2024-07-16 11:29:38 | ERROR | stderr |     from rrag.argument.parser import get_infer_args
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/__init__.py", line 8, in <module>
2024-07-16 11:29:38 | ERROR | stderr |     from .parser import get_eval_args, get_infer_args, get_train_args
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 47, in <module>
2024-07-16 11:29:38 | ERROR | stderr |     check_dependencies()
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/utils/misc.py", line 78, in check_dependencies
2024-07-16 11:29:38 | ERROR | stderr |     require_version("peft>=0.11.1", "To fix: pip install peft>=0.11.1")
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 111, in require_version
2024-07-16 11:29:38 | ERROR | stderr |     _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
2024-07-16 11:29:38 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 44, in _compare_versions
2024-07-16 11:29:38 | ERROR | stderr |     raise ImportError(
2024-07-16 11:29:38 | ERROR | stderr | ImportError: peft>=0.11.1 is required for a normal functioning of this module, but found peft==0.10.0.
2024-07-16 11:29:38 | ERROR | stderr | To fix: pip install peft>=0.11.1
2024-07-16 11:29:54 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 102, in require_version
2024-07-16 11:29:54 | ERROR | stderr |     got_ver = importlib.metadata.version(pkg)
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/importlib/metadata/__init__.py", line 996, in version
2024-07-16 11:29:54 | ERROR | stderr |     return distribution(distribution_name).version
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/importlib/metadata/__init__.py", line 969, in distribution
2024-07-16 11:29:54 | ERROR | stderr |     return Distribution.from_name(distribution_name)
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/importlib/metadata/__init__.py", line 548, in from_name
2024-07-16 11:29:54 | ERROR | stderr |     raise PackageNotFoundError(name)
2024-07-16 11:29:54 | ERROR | stderr | importlib.metadata.PackageNotFoundError: No package metadata was found for trl
2024-07-16 11:29:54 | ERROR | stderr | 
2024-07-16 11:29:54 | ERROR | stderr | During handling of the above exception, another exception occurred:
2024-07-16 11:29:54 | ERROR | stderr | 
2024-07-16 11:29:54 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 6, in <module>
2024-07-16 11:29:54 | ERROR | stderr |     from rrag.argument.parser import get_infer_args
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/__init__.py", line 8, in <module>
2024-07-16 11:29:54 | ERROR | stderr |     from .parser import get_eval_args, get_infer_args, get_train_args
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 47, in <module>
2024-07-16 11:29:54 | ERROR | stderr |     check_dependencies()
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/utils/misc.py", line 79, in check_dependencies
2024-07-16 11:29:54 | ERROR | stderr |     require_version("trl>=0.8.6", "To fix: pip install trl>=0.8.6")
2024-07-16 11:29:54 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 104, in require_version
2024-07-16 11:29:54 | ERROR | stderr |     raise importlib.metadata.PackageNotFoundError(
2024-07-16 11:29:54 | ERROR | stderr | importlib.metadata.PackageNotFoundError: No package metadata was found for The 'trl>=0.8.6' distribution was not found and is required by this application.
2024-07-16 11:29:54 | ERROR | stderr | To fix: pip install trl>=0.8.6
2024-07-16 11:41:19 | INFO | stdout | [36m(RayWorkerVllm pid=35724)[0m INFO 07-16 11:41:19 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
2024-07-16 11:41:19 | INFO | stdout | [36m(RayWorkerVllm pid=35724)[0m INFO 07-16 11:41:19 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
2024-07-16 11:41:24 | INFO | stdout | [36m(RayWorkerVllm pid=35724)[0m INFO 07-16 11:41:24 model_runner.py:756] Graph capturing finished in 5 secs.
2024-07-16 11:41:24 | INFO | stdout | [36m(RayWorkerVllm pid=40813)[0m INFO 07-16 11:41:19 model_runner.py:684] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)[0m
2024-07-16 11:41:24 | INFO | stdout | [36m(RayWorkerVllm pid=40813)[0m INFO 07-16 11:41:19 model_runner.py:688] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.[32m [repeated 2x across cluster][0m
2024-07-16 11:41:25 | ERROR | stderr | Processed prompts:   0%|                                                                                                                                                                                 | 0/30 [00:00<?, ?it/s]
2024-07-16 11:41:29 | ERROR | stderr | Processed prompts:   3%|█████▋                                                                                                                                                                   | 1/30 [00:04<02:20,  4.84s/it]
2024-07-16 11:41:30 | ERROR | stderr | Processed prompts:   7%|███████████▎                                                                                                                                                             | 2/30 [00:05<01:07,  2.40s/it]
2024-07-16 11:41:30 | ERROR | stderr | Processed prompts:  30%|██████████████████████████████████████████████████▋                                                                                                                      | 9/30 [00:05<00:07,  2.74it/s]
2024-07-16 11:41:31 | ERROR | stderr | Processed prompts:  57%|███████████████████████████████████████████████████████████████████████████████████████████████▏                                                                        | 17/30 [00:06<00:02,  5.00it/s]
2024-07-16 11:41:31 | ERROR | stderr | Processed prompts:  67%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                                                        | 20/30 [00:06<00:01,  5.73it/s]
2024-07-16 11:41:31 | ERROR | stderr | Processed prompts:  73%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                            | 22/30 [00:06<00:01,  6.48it/s]
2024-07-16 11:41:31 | ERROR | stderr | Processed prompts:  87%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                      | 26/30 [00:06<00:00,  8.75it/s]
2024-07-16 11:41:32 | ERROR | stderr | Processed prompts:  97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 29/30 [00:07<00:00, 10.18it/s]
2024-07-16 11:41:32 | ERROR | stderr | Processed prompts: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:07<00:00,  4.01it/s]
2024-07-16 11:41:32 | ERROR | stderr | 
2024-07-16 11:41:32 | ERROR | stderr | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH [--adapter_name_or_path ADAPTER_NAME_OR_PATH] [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR] [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 11:41:32 | ERROR | stderr |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]] [--split_special_tokens [SPLIT_SPECIAL_TOKENS]] [--new_special_tokens NEW_SPECIAL_TOKENS] [--model_revision MODEL_REVISION]
2024-07-16 11:41:32 | ERROR | stderr |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]] [--no_low_cpu_mem_usage] [--quantization_method {bitsandbytes,hqq,eetq}] [--quantization_bit QUANTIZATION_BIT] [--quantization_type {fp4,nf4}]
2024-07-16 11:41:32 | ERROR | stderr |               [--double_quantization [DOUBLE_QUANTIZATION]] [--no_double_quantization] [--quantization_device_map {auto}] [--rope_scaling {linear,dynamic}] [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 11:41:32 | ERROR | stderr |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}] [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]] [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 11:41:32 | ERROR | stderr |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]] [--upcast_layernorm [UPCAST_LAYERNORM]] [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]] [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 11:41:32 | ERROR | stderr |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL] [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]] [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 11:41:32 | ERROR | stderr |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]] [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}] [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 11:41:32 | ERROR | stderr |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE] [--export_device {cpu,auto}] [--export_quantization_bit EXPORT_QUANTIZATION_BIT] [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 11:41:32 | ERROR | stderr |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES] [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN] [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 11:41:32 | ERROR | stderr |               [--export_hub_model_id EXPORT_HUB_MODEL_ID] [--print_param_status [PRINT_PARAM_STATUS]] [--template TEMPLATE] [--dataset DATASET] [--dataset_dir DATASET_DIR] [--split SPLIT] [--cutoff_len CUTOFF_LEN]
2024-07-16 11:41:32 | ERROR | stderr |               [--reserved_label_len RESERVED_LABEL_LEN] [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE] [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 11:41:32 | ERROR | stderr |               [--interleave_probs INTERLEAVE_PROBS] [--overwrite_cache [OVERWRITE_CACHE]] [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS] [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 11:41:32 | ERROR | stderr |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]] [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE] [--packing PACKING] [--tool_format TOOL_FORMAT] [--tokenized_path TOKENIZED_PATH]
2024-07-16 11:41:32 | ERROR | stderr |               [--do_sample [DO_SAMPLE]] [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K] [--num_beams NUM_BEAMS] [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 11:41:32 | ERROR | stderr |               [--repetition_penalty REPETITION_PENALTY] [--length_penalty LENGTH_PENALTY] [--default_system DEFAULT_SYSTEM]
2024-07-16 11:41:32 | ERROR | stderr | tmp.py: error: the following arguments are required: --model_name_or_path
2024-07-16 11:41:33 | INFO | stdout | [0m[36m(RayWorkerVllm pid=40564)[0m INFO 07-16 11:41:24 model_runner.py:756] Graph capturing finished in 5 secs.[32m [repeated 2x across cluster][0m
2024-07-16 11:42:18 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:42:18 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:42:18 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:42:18 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:42:18 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:42:18 | ERROR | stderr | ValueError: not enough values to unpack (expected 4, got 3)
2024-07-16 11:48:02 | INFO | stdout | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH [--adapter_name_or_path ADAPTER_NAME_OR_PATH] [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR] [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 11:48:02 | INFO | stdout |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]] [--split_special_tokens [SPLIT_SPECIAL_TOKENS]] [--new_special_tokens NEW_SPECIAL_TOKENS] [--model_revision MODEL_REVISION]
2024-07-16 11:48:02 | INFO | stdout |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]] [--no_low_cpu_mem_usage] [--quantization_method {bitsandbytes,hqq,eetq}] [--quantization_bit QUANTIZATION_BIT] [--quantization_type {fp4,nf4}]
2024-07-16 11:48:02 | INFO | stdout |               [--double_quantization [DOUBLE_QUANTIZATION]] [--no_double_quantization] [--quantization_device_map {auto}] [--rope_scaling {linear,dynamic}] [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 11:48:02 | INFO | stdout |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}] [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]] [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 11:48:02 | INFO | stdout |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]] [--upcast_layernorm [UPCAST_LAYERNORM]] [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]] [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 11:48:02 | INFO | stdout |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL] [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]] [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 11:48:02 | INFO | stdout |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]] [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}] [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 11:48:02 | INFO | stdout |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE] [--export_device {cpu,auto}] [--export_quantization_bit EXPORT_QUANTIZATION_BIT] [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 11:48:02 | INFO | stdout |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES] [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN] [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 11:48:02 | INFO | stdout |               [--export_hub_model_id EXPORT_HUB_MODEL_ID] [--print_param_status [PRINT_PARAM_STATUS]] [--template TEMPLATE] [--dataset DATASET] [--dataset_dir DATASET_DIR] [--split SPLIT] [--cutoff_len CUTOFF_LEN]
2024-07-16 11:48:02 | INFO | stdout |               [--reserved_label_len RESERVED_LABEL_LEN] [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE] [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 11:48:02 | INFO | stdout |               [--interleave_probs INTERLEAVE_PROBS] [--overwrite_cache [OVERWRITE_CACHE]] [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS] [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 11:48:02 | INFO | stdout |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]] [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE] [--packing PACKING] [--tool_format TOOL_FORMAT] [--tokenized_path TOKENIZED_PATH]
2024-07-16 11:48:02 | INFO | stdout |               [--do_sample [DO_SAMPLE]] [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K] [--num_beams NUM_BEAMS] [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 11:48:02 | INFO | stdout |               [--repetition_penalty REPETITION_PENALTY] [--length_penalty LENGTH_PENALTY] [--default_system DEFAULT_SYSTEM]
2024-07-16 11:48:02 | INFO | stdout | 
2024-07-16 11:48:02 | INFO | stdout | options:
2024-07-16 11:48:02 | INFO | stdout |   -h, --help            show this help message and exit
2024-07-16 11:48:02 | INFO | stdout |   --model_name_or_path MODEL_NAME_OR_PATH
2024-07-16 11:48:02 | INFO | stdout |                         Path to the model weight or identifier from huggingface.co/models or modelscope.cn/models. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --adapter_name_or_path ADAPTER_NAME_OR_PATH
2024-07-16 11:48:02 | INFO | stdout |                         Path to the adapter weight or identifier from huggingface.co/models. Use commas to separate multiple adapters. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --adapter_folder ADAPTER_FOLDER
2024-07-16 11:48:02 | INFO | stdout |                         The folder containing the adapter weights to load. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --cache_dir CACHE_DIR
2024-07-16 11:48:02 | INFO | stdout |                         Where to store the pre-trained models downloaded from huggingface.co or modelscope.cn. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --use_fast_tokenizer [USE_FAST_TOKENIZER]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_use_fast_tokenizer
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --resize_vocab [RESIZE_VOCAB]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to resize the tokenizer vocab and the embedding layers. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --split_special_tokens [SPLIT_SPECIAL_TOKENS]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not the special tokens should be split during the tokenization process. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --new_special_tokens NEW_SPECIAL_TOKENS
2024-07-16 11:48:02 | INFO | stdout |                         Special tokens to be added into the tokenizer. Use commas to separate multiple tokens. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --model_revision MODEL_REVISION
2024-07-16 11:48:02 | INFO | stdout |                         The specific model version to use (can be a branch name, tag name or commit id). (default: main)
2024-07-16 11:48:02 | INFO | stdout |   --low_cpu_mem_usage [LOW_CPU_MEM_USAGE]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_low_cpu_mem_usage
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --quantization_method {bitsandbytes,hqq,eetq}
2024-07-16 11:48:02 | INFO | stdout |                         Quantization method to use for on-the-fly quantization. (default: bitsandbytes)
2024-07-16 11:48:02 | INFO | stdout |   --quantization_bit QUANTIZATION_BIT
2024-07-16 11:48:02 | INFO | stdout |                         The number of bits to quantize the model using bitsandbytes. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --quantization_type {fp4,nf4}
2024-07-16 11:48:02 | INFO | stdout |                         Quantization data type to use in int4 training. (default: nf4)
2024-07-16 11:48:02 | INFO | stdout |   --double_quantization [DOUBLE_QUANTIZATION]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_double_quantization
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --quantization_device_map {auto}
2024-07-16 11:48:02 | INFO | stdout |                         Device map used to infer the 4-bit quantized model, needs bitsandbytes>=0.43.0. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --rope_scaling {linear,dynamic}
2024-07-16 11:48:02 | INFO | stdout |                         Which scaling strategy should be adopted for the RoPE embeddings. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --flash_attn {auto,disabled,sdpa,fa2}
2024-07-16 11:48:02 | INFO | stdout |                         Enable FlashAttention for faster training and inference. (default: auto)
2024-07-16 11:48:02 | INFO | stdout |   --shift_attn [SHIFT_ATTN]
2024-07-16 11:48:02 | INFO | stdout |                         Enable shift short attention (S^2-Attn) proposed by LongLoRA. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --mixture_of_depths {convert,load}
2024-07-16 11:48:02 | INFO | stdout |                         Convert the model to mixture-of-depths (MoD) or load the MoD model. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --use_unsloth [USE_UNSLOTH]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use unsloth's optimization for the LoRA training. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --visual_inputs [VISUAL_INPUTS]
2024-07-16 11:48:02 | INFO | stdout |                         Whethor or not to use multimodal LLM that accepts visual inputs. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --moe_aux_loss_coef MOE_AUX_LOSS_COEF
2024-07-16 11:48:02 | INFO | stdout |                         Coefficient of the auxiliary router loss in mixture-of-experts model. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to disable gradient checkpointing. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --upcast_layernorm [UPCAST_LAYERNORM]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to upcast the layernorm weights in fp32. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to upcast the output of lm_head in fp32. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --train_from_scratch [TRAIN_FROM_SCRATCH]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to randomly initialize the model weights. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --infer_backend {huggingface,vllm}
2024-07-16 11:48:02 | INFO | stdout |                         Backend engine used at inference. (default: huggingface)
2024-07-16 11:48:02 | INFO | stdout |   --vllm_maxlen VLLM_MAXLEN
2024-07-16 11:48:02 | INFO | stdout |                         Maximum sequence (prompt + response) length of the vLLM engine. (default: 2048)
2024-07-16 11:48:02 | INFO | stdout |   --vllm_gpu_util VLLM_GPU_UTIL
2024-07-16 11:48:02 | INFO | stdout |                         The fraction of GPU memory in (0,1) to be used for the vLLM engine. (default: 0.9)
2024-07-16 11:48:02 | INFO | stdout |   --vllm_enforce_eager [VLLM_ENFORCE_EAGER]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to disable CUDA graph in the vLLM engine. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --vllm_max_lora_rank VLLM_MAX_LORA_RANK
2024-07-16 11:48:02 | INFO | stdout |                         Maximum rank of all LoRAs in the vLLM engine. (default: 32)
2024-07-16 11:48:02 | INFO | stdout |   --offload_folder OFFLOAD_FOLDER
2024-07-16 11:48:02 | INFO | stdout |                         Path to offload model weights. (default: offload)
2024-07-16 11:48:02 | INFO | stdout |   --use_cache [USE_CACHE]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use KV cache in generation. (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_use_cache        Whether or not to use KV cache in generation. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --infer_dtype {auto,float16,bfloat16,float32}
2024-07-16 11:48:02 | INFO | stdout |                         Data type for model weights and activations at inference. (default: auto)
2024-07-16 11:48:02 | INFO | stdout |   --hf_hub_token HF_HUB_TOKEN
2024-07-16 11:48:02 | INFO | stdout |                         Auth token to log in with Hugging Face Hub. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --ms_hub_token MS_HUB_TOKEN
2024-07-16 11:48:02 | INFO | stdout |                         Auth token to log in with ModelScope Hub. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --export_dir EXPORT_DIR
2024-07-16 11:48:02 | INFO | stdout |                         Path to the directory to save the exported model. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --export_size EXPORT_SIZE
2024-07-16 11:48:02 | INFO | stdout |                         The file shard size (in GB) of the exported model. (default: 1)
2024-07-16 11:48:02 | INFO | stdout |   --export_device {cpu,auto}
2024-07-16 11:48:02 | INFO | stdout |                         The device used in model export, use `auto` to accelerate exporting. (default: cpu)
2024-07-16 11:48:02 | INFO | stdout |   --export_quantization_bit EXPORT_QUANTIZATION_BIT
2024-07-16 11:48:02 | INFO | stdout |                         The number of bits to quantize the exported model. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --export_quantization_dataset EXPORT_QUANTIZATION_DATASET
2024-07-16 11:48:02 | INFO | stdout |                         Path to the dataset or dataset name to use in quantizing the exported model. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES
2024-07-16 11:48:02 | INFO | stdout |                         The number of samples used for quantization. (default: 128)
2024-07-16 11:48:02 | INFO | stdout |   --export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN
2024-07-16 11:48:02 | INFO | stdout |                         The maximum length of the model inputs used for quantization. (default: 1024)
2024-07-16 11:48:02 | INFO | stdout |   --export_legacy_format [EXPORT_LEGACY_FORMAT]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to save the `.bin` files instead of `.safetensors`. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --export_hub_model_id EXPORT_HUB_MODEL_ID
2024-07-16 11:48:02 | INFO | stdout |                         The name of the repository if push the model to the Hugging Face hub. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --print_param_status [PRINT_PARAM_STATUS]
2024-07-16 11:48:02 | INFO | stdout |                         For debugging purposes, print the status of the parameters in the model. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --template TEMPLATE   Which template to use for constructing prompts in training and inference. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --dataset DATASET     The name of provided dataset(s) to use. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --dataset_dir DATASET_DIR
2024-07-16 11:48:02 | INFO | stdout |                         Path to the folder containing the datasets. (default: data)
2024-07-16 11:48:02 | INFO | stdout |   --split SPLIT         Which dataset split to use for training and evaluation. (default: train)
2024-07-16 11:48:02 | INFO | stdout |   --cutoff_len CUTOFF_LEN
2024-07-16 11:48:02 | INFO | stdout |                         The cutoff length of the tokenized inputs in the dataset. (default: 4096)
2024-07-16 11:48:02 | INFO | stdout |   --reserved_label_len RESERVED_LABEL_LEN
2024-07-16 11:48:02 | INFO | stdout |                         The minimum cutoff length reserved for the tokenized labels in the dataset. (default: 1)
2024-07-16 11:48:02 | INFO | stdout |   --train_on_prompt [TRAIN_ON_PROMPT]
2024-07-16 11:48:02 | INFO | stdout |                         Whether to disable the mask on the prompt or not. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --streaming [STREAMING]
2024-07-16 11:48:02 | INFO | stdout |                         Enable dataset streaming. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --buffer_size BUFFER_SIZE
2024-07-16 11:48:02 | INFO | stdout |                         Size of the buffer to randomly sample examples from in dataset streaming. (default: 16384)
2024-07-16 11:48:02 | INFO | stdout |   --mix_strategy {concat,interleave_under,interleave_over}
2024-07-16 11:48:02 | INFO | stdout |                         Strategy to use in dataset mixing (concat/interleave) (undersampling/oversampling). (default: concat)
2024-07-16 11:48:02 | INFO | stdout |   --interleave_probs INTERLEAVE_PROBS
2024-07-16 11:48:02 | INFO | stdout |                         Probabilities to sample data from datasets. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --overwrite_cache [OVERWRITE_CACHE]
2024-07-16 11:48:02 | INFO | stdout |                         Overwrite the cached training and evaluation sets. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --preprocessing_num_workers PREPROCESSING_NUM_WORKERS
2024-07-16 11:48:02 | INFO | stdout |                         The number of processes to use for the pre-processing. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --max_samples MAX_SAMPLES
2024-07-16 11:48:02 | INFO | stdout |                         For debugging purposes, truncate the number of examples for each dataset. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --eval_num_beams EVAL_NUM_BEAMS
2024-07-16 11:48:02 | INFO | stdout |                         Number of beams to use for evaluation. This argument will be passed to `model.generate` (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_ignore_pad_token_for_loss
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --val_size VAL_SIZE   Size of the development set, should be an integer or a float in range `[0,1)`. (default: 0.0)
2024-07-16 11:48:02 | INFO | stdout |   --packing PACKING     Whether or not to pack the sequences in training. Will automatically enable in pre-training. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --tool_format TOOL_FORMAT
2024-07-16 11:48:02 | INFO | stdout |                         Tool format to use for constructing function calling examples. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --tokenized_path TOKENIZED_PATH
2024-07-16 11:48:02 | INFO | stdout |                         Path to save or load the tokenized datasets. (default: None)
2024-07-16 11:48:02 | INFO | stdout |   --do_sample [DO_SAMPLE]
2024-07-16 11:48:02 | INFO | stdout |                         Whether or not to use sampling, use greedy decoding otherwise. (default: True)
2024-07-16 11:48:02 | INFO | stdout |   --no_do_sample        Whether or not to use sampling, use greedy decoding otherwise. (default: False)
2024-07-16 11:48:02 | INFO | stdout |   --temperature TEMPERATURE
2024-07-16 11:48:02 | INFO | stdout |                         The value used to modulate the next token probabilities. (default: 0.95)
2024-07-16 11:48:02 | INFO | stdout |   --top_p TOP_P         The smallest set of most probable tokens with probabilities that add up to top_p or higher are kept. (default: 0.7)
2024-07-16 11:48:02 | INFO | stdout |   --top_k TOP_K         The number of highest probability vocabulary tokens to keep for top-k filtering. (default: 50)
2024-07-16 11:48:02 | INFO | stdout |   --num_beams NUM_BEAMS
2024-07-16 11:48:02 | INFO | stdout |                         Number of beams for beam search. 1 means no beam search. (default: 1)
2024-07-16 11:48:02 | INFO | stdout |   --max_length MAX_LENGTH
2024-07-16 11:48:02 | INFO | stdout |                         The maximum length the generated tokens can have. It can be overridden by max_new_tokens. (default: 1024)
2024-07-16 11:48:02 | INFO | stdout |   --max_new_tokens MAX_NEW_TOKENS
2024-07-16 11:48:02 | INFO | stdout |                         The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. (default: 1024)
2024-07-16 11:48:02 | INFO | stdout |   --repetition_penalty REPETITION_PENALTY
2024-07-16 11:48:02 | INFO | stdout |                         The parameter for repetition penalty. 1.0 means no penalty. (default: 1.0)
2024-07-16 11:48:02 | INFO | stdout |   --length_penalty LENGTH_PENALTY
2024-07-16 11:48:02 | INFO | stdout |                         Exponential penalty to the length that is used with beam-based generation. (default: 1.0)
2024-07-16 11:48:02 | INFO | stdout |   --default_system DEFAULT_SYSTEM
2024-07-16 11:48:02 | INFO | stdout |                         Default system message to use in chat completion. (default: None)
2024-07-16 11:48:02 | INFO | stdout | 
2024-07-16 11:48:02 | INFO | stdout | Got unknown args, potentially deprecated arguments: ['temperature', '0.7']
2024-07-16 11:48:02 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:48:02 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:48:02 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:48:02 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:48:02 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:48:02 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 141, in _parse_infer_args
2024-07-16 11:48:02 | ERROR | stderr |     return _parse_args(parser, args)
2024-07-16 11:48:02 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 73, in _parse_args
2024-07-16 11:48:02 | ERROR | stderr |     raise ValueError("Some specified arguments are not used by the HfArgumentParser: {}".format(unknown_args))
2024-07-16 11:48:02 | ERROR | stderr | ValueError: Some specified arguments are not used by the HfArgumentParser: ['temperature', '0.7']
2024-07-16 11:48:30 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:48:30 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:48:30 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:48:30 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:48:30 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:48:30 | ERROR | stderr | ValueError: not enough values to unpack (expected 4, got 3)
2024-07-16 11:48:52 | INFO | stdout | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH [--adapter_name_or_path ADAPTER_NAME_OR_PATH] [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR] [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 11:48:52 | INFO | stdout |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]] [--split_special_tokens [SPLIT_SPECIAL_TOKENS]] [--new_special_tokens NEW_SPECIAL_TOKENS] [--model_revision MODEL_REVISION]
2024-07-16 11:48:52 | INFO | stdout |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]] [--no_low_cpu_mem_usage] [--quantization_method {bitsandbytes,hqq,eetq}] [--quantization_bit QUANTIZATION_BIT] [--quantization_type {fp4,nf4}]
2024-07-16 11:48:52 | INFO | stdout |               [--double_quantization [DOUBLE_QUANTIZATION]] [--no_double_quantization] [--quantization_device_map {auto}] [--rope_scaling {linear,dynamic}] [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 11:48:52 | INFO | stdout |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}] [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]] [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 11:48:52 | INFO | stdout |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]] [--upcast_layernorm [UPCAST_LAYERNORM]] [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]] [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 11:48:52 | INFO | stdout |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL] [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]] [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 11:48:52 | INFO | stdout |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]] [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}] [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 11:48:52 | INFO | stdout |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE] [--export_device {cpu,auto}] [--export_quantization_bit EXPORT_QUANTIZATION_BIT] [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 11:48:52 | INFO | stdout |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES] [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN] [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 11:48:52 | INFO | stdout |               [--export_hub_model_id EXPORT_HUB_MODEL_ID] [--print_param_status [PRINT_PARAM_STATUS]] [--template TEMPLATE] [--dataset DATASET] [--dataset_dir DATASET_DIR] [--split SPLIT] [--cutoff_len CUTOFF_LEN]
2024-07-16 11:48:52 | INFO | stdout |               [--reserved_label_len RESERVED_LABEL_LEN] [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE] [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 11:48:52 | INFO | stdout |               [--interleave_probs INTERLEAVE_PROBS] [--overwrite_cache [OVERWRITE_CACHE]] [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS] [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 11:48:52 | INFO | stdout |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]] [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE] [--packing PACKING] [--tool_format TOOL_FORMAT] [--tokenized_path TOKENIZED_PATH]
2024-07-16 11:48:52 | INFO | stdout |               [--do_sample [DO_SAMPLE]] [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K] [--num_beams NUM_BEAMS] [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 11:48:52 | INFO | stdout |               [--repetition_penalty REPETITION_PENALTY] [--length_penalty LENGTH_PENALTY] [--default_system DEFAULT_SYSTEM]
2024-07-16 11:48:52 | INFO | stdout | 
2024-07-16 11:48:52 | INFO | stdout | options:
2024-07-16 11:48:52 | INFO | stdout |   -h, --help            show this help message and exit
2024-07-16 11:48:52 | INFO | stdout |   --model_name_or_path MODEL_NAME_OR_PATH
2024-07-16 11:48:52 | INFO | stdout |                         Path to the model weight or identifier from huggingface.co/models or modelscope.cn/models. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --adapter_name_or_path ADAPTER_NAME_OR_PATH
2024-07-16 11:48:52 | INFO | stdout |                         Path to the adapter weight or identifier from huggingface.co/models. Use commas to separate multiple adapters. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --adapter_folder ADAPTER_FOLDER
2024-07-16 11:48:52 | INFO | stdout |                         The folder containing the adapter weights to load. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --cache_dir CACHE_DIR
2024-07-16 11:48:52 | INFO | stdout |                         Where to store the pre-trained models downloaded from huggingface.co or modelscope.cn. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --use_fast_tokenizer [USE_FAST_TOKENIZER]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_use_fast_tokenizer
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --resize_vocab [RESIZE_VOCAB]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to resize the tokenizer vocab and the embedding layers. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --split_special_tokens [SPLIT_SPECIAL_TOKENS]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not the special tokens should be split during the tokenization process. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --new_special_tokens NEW_SPECIAL_TOKENS
2024-07-16 11:48:52 | INFO | stdout |                         Special tokens to be added into the tokenizer. Use commas to separate multiple tokens. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --model_revision MODEL_REVISION
2024-07-16 11:48:52 | INFO | stdout |                         The specific model version to use (can be a branch name, tag name or commit id). (default: main)
2024-07-16 11:48:52 | INFO | stdout |   --low_cpu_mem_usage [LOW_CPU_MEM_USAGE]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_low_cpu_mem_usage
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --quantization_method {bitsandbytes,hqq,eetq}
2024-07-16 11:48:52 | INFO | stdout |                         Quantization method to use for on-the-fly quantization. (default: bitsandbytes)
2024-07-16 11:48:52 | INFO | stdout |   --quantization_bit QUANTIZATION_BIT
2024-07-16 11:48:52 | INFO | stdout |                         The number of bits to quantize the model using bitsandbytes. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --quantization_type {fp4,nf4}
2024-07-16 11:48:52 | INFO | stdout |                         Quantization data type to use in int4 training. (default: nf4)
2024-07-16 11:48:52 | INFO | stdout |   --double_quantization [DOUBLE_QUANTIZATION]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_double_quantization
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --quantization_device_map {auto}
2024-07-16 11:48:52 | INFO | stdout |                         Device map used to infer the 4-bit quantized model, needs bitsandbytes>=0.43.0. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --rope_scaling {linear,dynamic}
2024-07-16 11:48:52 | INFO | stdout |                         Which scaling strategy should be adopted for the RoPE embeddings. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --flash_attn {auto,disabled,sdpa,fa2}
2024-07-16 11:48:52 | INFO | stdout |                         Enable FlashAttention for faster training and inference. (default: auto)
2024-07-16 11:48:52 | INFO | stdout |   --shift_attn [SHIFT_ATTN]
2024-07-16 11:48:52 | INFO | stdout |                         Enable shift short attention (S^2-Attn) proposed by LongLoRA. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --mixture_of_depths {convert,load}
2024-07-16 11:48:52 | INFO | stdout |                         Convert the model to mixture-of-depths (MoD) or load the MoD model. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --use_unsloth [USE_UNSLOTH]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use unsloth's optimization for the LoRA training. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --visual_inputs [VISUAL_INPUTS]
2024-07-16 11:48:52 | INFO | stdout |                         Whethor or not to use multimodal LLM that accepts visual inputs. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --moe_aux_loss_coef MOE_AUX_LOSS_COEF
2024-07-16 11:48:52 | INFO | stdout |                         Coefficient of the auxiliary router loss in mixture-of-experts model. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to disable gradient checkpointing. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --upcast_layernorm [UPCAST_LAYERNORM]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to upcast the layernorm weights in fp32. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to upcast the output of lm_head in fp32. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --train_from_scratch [TRAIN_FROM_SCRATCH]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to randomly initialize the model weights. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --infer_backend {huggingface,vllm}
2024-07-16 11:48:52 | INFO | stdout |                         Backend engine used at inference. (default: huggingface)
2024-07-16 11:48:52 | INFO | stdout |   --vllm_maxlen VLLM_MAXLEN
2024-07-16 11:48:52 | INFO | stdout |                         Maximum sequence (prompt + response) length of the vLLM engine. (default: 2048)
2024-07-16 11:48:52 | INFO | stdout |   --vllm_gpu_util VLLM_GPU_UTIL
2024-07-16 11:48:52 | INFO | stdout |                         The fraction of GPU memory in (0,1) to be used for the vLLM engine. (default: 0.9)
2024-07-16 11:48:52 | INFO | stdout |   --vllm_enforce_eager [VLLM_ENFORCE_EAGER]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to disable CUDA graph in the vLLM engine. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --vllm_max_lora_rank VLLM_MAX_LORA_RANK
2024-07-16 11:48:52 | INFO | stdout |                         Maximum rank of all LoRAs in the vLLM engine. (default: 32)
2024-07-16 11:48:52 | INFO | stdout |   --offload_folder OFFLOAD_FOLDER
2024-07-16 11:48:52 | INFO | stdout |                         Path to offload model weights. (default: offload)
2024-07-16 11:48:52 | INFO | stdout |   --use_cache [USE_CACHE]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use KV cache in generation. (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_use_cache        Whether or not to use KV cache in generation. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --infer_dtype {auto,float16,bfloat16,float32}
2024-07-16 11:48:52 | INFO | stdout |                         Data type for model weights and activations at inference. (default: auto)
2024-07-16 11:48:52 | INFO | stdout |   --hf_hub_token HF_HUB_TOKEN
2024-07-16 11:48:52 | INFO | stdout |                         Auth token to log in with Hugging Face Hub. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --ms_hub_token MS_HUB_TOKEN
2024-07-16 11:48:52 | INFO | stdout |                         Auth token to log in with ModelScope Hub. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --export_dir EXPORT_DIR
2024-07-16 11:48:52 | INFO | stdout |                         Path to the directory to save the exported model. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --export_size EXPORT_SIZE
2024-07-16 11:48:52 | INFO | stdout |                         The file shard size (in GB) of the exported model. (default: 1)
2024-07-16 11:48:52 | INFO | stdout |   --export_device {cpu,auto}
2024-07-16 11:48:52 | INFO | stdout |                         The device used in model export, use `auto` to accelerate exporting. (default: cpu)
2024-07-16 11:48:52 | INFO | stdout |   --export_quantization_bit EXPORT_QUANTIZATION_BIT
2024-07-16 11:48:52 | INFO | stdout |                         The number of bits to quantize the exported model. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --export_quantization_dataset EXPORT_QUANTIZATION_DATASET
2024-07-16 11:48:52 | INFO | stdout |                         Path to the dataset or dataset name to use in quantizing the exported model. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES
2024-07-16 11:48:52 | INFO | stdout |                         The number of samples used for quantization. (default: 128)
2024-07-16 11:48:52 | INFO | stdout |   --export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN
2024-07-16 11:48:52 | INFO | stdout |                         The maximum length of the model inputs used for quantization. (default: 1024)
2024-07-16 11:48:52 | INFO | stdout |   --export_legacy_format [EXPORT_LEGACY_FORMAT]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to save the `.bin` files instead of `.safetensors`. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --export_hub_model_id EXPORT_HUB_MODEL_ID
2024-07-16 11:48:52 | INFO | stdout |                         The name of the repository if push the model to the Hugging Face hub. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --print_param_status [PRINT_PARAM_STATUS]
2024-07-16 11:48:52 | INFO | stdout |                         For debugging purposes, print the status of the parameters in the model. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --template TEMPLATE   Which template to use for constructing prompts in training and inference. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --dataset DATASET     The name of provided dataset(s) to use. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --dataset_dir DATASET_DIR
2024-07-16 11:48:52 | INFO | stdout |                         Path to the folder containing the datasets. (default: data)
2024-07-16 11:48:52 | INFO | stdout |   --split SPLIT         Which dataset split to use for training and evaluation. (default: train)
2024-07-16 11:48:52 | INFO | stdout |   --cutoff_len CUTOFF_LEN
2024-07-16 11:48:52 | INFO | stdout |                         The cutoff length of the tokenized inputs in the dataset. (default: 4096)
2024-07-16 11:48:52 | INFO | stdout |   --reserved_label_len RESERVED_LABEL_LEN
2024-07-16 11:48:52 | INFO | stdout |                         The minimum cutoff length reserved for the tokenized labels in the dataset. (default: 1)
2024-07-16 11:48:52 | INFO | stdout |   --train_on_prompt [TRAIN_ON_PROMPT]
2024-07-16 11:48:52 | INFO | stdout |                         Whether to disable the mask on the prompt or not. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --streaming [STREAMING]
2024-07-16 11:48:52 | INFO | stdout |                         Enable dataset streaming. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --buffer_size BUFFER_SIZE
2024-07-16 11:48:52 | INFO | stdout |                         Size of the buffer to randomly sample examples from in dataset streaming. (default: 16384)
2024-07-16 11:48:52 | INFO | stdout |   --mix_strategy {concat,interleave_under,interleave_over}
2024-07-16 11:48:52 | INFO | stdout |                         Strategy to use in dataset mixing (concat/interleave) (undersampling/oversampling). (default: concat)
2024-07-16 11:48:52 | INFO | stdout |   --interleave_probs INTERLEAVE_PROBS
2024-07-16 11:48:52 | INFO | stdout |                         Probabilities to sample data from datasets. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --overwrite_cache [OVERWRITE_CACHE]
2024-07-16 11:48:52 | INFO | stdout |                         Overwrite the cached training and evaluation sets. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --preprocessing_num_workers PREPROCESSING_NUM_WORKERS
2024-07-16 11:48:52 | INFO | stdout |                         The number of processes to use for the pre-processing. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --max_samples MAX_SAMPLES
2024-07-16 11:48:52 | INFO | stdout |                         For debugging purposes, truncate the number of examples for each dataset. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --eval_num_beams EVAL_NUM_BEAMS
2024-07-16 11:48:52 | INFO | stdout |                         Number of beams to use for evaluation. This argument will be passed to `model.generate` (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_ignore_pad_token_for_loss
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --val_size VAL_SIZE   Size of the development set, should be an integer or a float in range `[0,1)`. (default: 0.0)
2024-07-16 11:48:52 | INFO | stdout |   --packing PACKING     Whether or not to pack the sequences in training. Will automatically enable in pre-training. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --tool_format TOOL_FORMAT
2024-07-16 11:48:52 | INFO | stdout |                         Tool format to use for constructing function calling examples. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --tokenized_path TOKENIZED_PATH
2024-07-16 11:48:52 | INFO | stdout |                         Path to save or load the tokenized datasets. (default: None)
2024-07-16 11:48:52 | INFO | stdout |   --do_sample [DO_SAMPLE]
2024-07-16 11:48:52 | INFO | stdout |                         Whether or not to use sampling, use greedy decoding otherwise. (default: True)
2024-07-16 11:48:52 | INFO | stdout |   --no_do_sample        Whether or not to use sampling, use greedy decoding otherwise. (default: False)
2024-07-16 11:48:52 | INFO | stdout |   --temperature TEMPERATURE
2024-07-16 11:48:52 | INFO | stdout |                         The value used to modulate the next token probabilities. (default: 0.95)
2024-07-16 11:48:52 | INFO | stdout |   --top_p TOP_P         The smallest set of most probable tokens with probabilities that add up to top_p or higher are kept. (default: 0.7)
2024-07-16 11:48:52 | INFO | stdout |   --top_k TOP_K         The number of highest probability vocabulary tokens to keep for top-k filtering. (default: 50)
2024-07-16 11:48:52 | INFO | stdout |   --num_beams NUM_BEAMS
2024-07-16 11:48:52 | INFO | stdout |                         Number of beams for beam search. 1 means no beam search. (default: 1)
2024-07-16 11:48:52 | INFO | stdout |   --max_length MAX_LENGTH
2024-07-16 11:48:52 | INFO | stdout |                         The maximum length the generated tokens can have. It can be overridden by max_new_tokens. (default: 1024)
2024-07-16 11:48:52 | INFO | stdout |   --max_new_tokens MAX_NEW_TOKENS
2024-07-16 11:48:52 | INFO | stdout |                         The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. (default: 1024)
2024-07-16 11:48:52 | INFO | stdout |   --repetition_penalty REPETITION_PENALTY
2024-07-16 11:48:52 | INFO | stdout |                         The parameter for repetition penalty. 1.0 means no penalty. (default: 1.0)
2024-07-16 11:48:52 | INFO | stdout |   --length_penalty LENGTH_PENALTY
2024-07-16 11:48:52 | INFO | stdout |                         Exponential penalty to the length that is used with beam-based generation. (default: 1.0)
2024-07-16 11:48:52 | INFO | stdout |   --default_system DEFAULT_SYSTEM
2024-07-16 11:48:52 | INFO | stdout |                         Default system message to use in chat completion. (default: None)
2024-07-16 11:48:52 | INFO | stdout | 
2024-07-16 11:48:52 | INFO | stdout | Got unknown args, potentially deprecated arguments: ['--n_shot', '5']
2024-07-16 11:48:52 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:48:52 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:48:52 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:48:52 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:48:52 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:48:52 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 141, in _parse_infer_args
2024-07-16 11:48:52 | ERROR | stderr |     return _parse_args(parser, args)
2024-07-16 11:48:52 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 73, in _parse_args
2024-07-16 11:48:52 | ERROR | stderr |     raise ValueError("Some specified arguments are not used by the HfArgumentParser: {}".format(unknown_args))
2024-07-16 11:48:52 | ERROR | stderr | ValueError: Some specified arguments are not used by the HfArgumentParser: ['--n_shot', '5']
2024-07-16 11:50:14 | INFO | stdout | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH [--adapter_name_or_path ADAPTER_NAME_OR_PATH] [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR] [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 11:50:14 | INFO | stdout |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]] [--split_special_tokens [SPLIT_SPECIAL_TOKENS]] [--new_special_tokens NEW_SPECIAL_TOKENS] [--model_revision MODEL_REVISION]
2024-07-16 11:50:14 | INFO | stdout |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]] [--no_low_cpu_mem_usage] [--quantization_method {bitsandbytes,hqq,eetq}] [--quantization_bit QUANTIZATION_BIT] [--quantization_type {fp4,nf4}]
2024-07-16 11:50:14 | INFO | stdout |               [--double_quantization [DOUBLE_QUANTIZATION]] [--no_double_quantization] [--quantization_device_map {auto}] [--rope_scaling {linear,dynamic}] [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 11:50:14 | INFO | stdout |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}] [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]] [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 11:50:14 | INFO | stdout |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]] [--upcast_layernorm [UPCAST_LAYERNORM]] [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]] [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 11:50:14 | INFO | stdout |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN] [--vllm_gpu_util VLLM_GPU_UTIL] [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]] [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 11:50:14 | INFO | stdout |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]] [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}] [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 11:50:14 | INFO | stdout |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE] [--export_device {cpu,auto}] [--export_quantization_bit EXPORT_QUANTIZATION_BIT] [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 11:50:14 | INFO | stdout |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES] [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN] [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 11:50:14 | INFO | stdout |               [--export_hub_model_id EXPORT_HUB_MODEL_ID] [--print_param_status [PRINT_PARAM_STATUS]] [--template TEMPLATE] [--dataset DATASET] [--dataset_dir DATASET_DIR] [--split SPLIT] [--cutoff_len CUTOFF_LEN]
2024-07-16 11:50:14 | INFO | stdout |               [--reserved_label_len RESERVED_LABEL_LEN] [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]] [--buffer_size BUFFER_SIZE] [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 11:50:14 | INFO | stdout |               [--interleave_probs INTERLEAVE_PROBS] [--overwrite_cache [OVERWRITE_CACHE]] [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS] [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 11:50:14 | INFO | stdout |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]] [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE] [--packing PACKING] [--tool_format TOOL_FORMAT] [--tokenized_path TOKENIZED_PATH]
2024-07-16 11:50:14 | INFO | stdout |               [--do_sample [DO_SAMPLE]] [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P] [--top_k TOP_K] [--num_beams NUM_BEAMS] [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 11:50:14 | INFO | stdout |               [--repetition_penalty REPETITION_PENALTY] [--length_penalty LENGTH_PENALTY] [--default_system DEFAULT_SYSTEM]
2024-07-16 11:50:14 | INFO | stdout | 
2024-07-16 11:50:14 | INFO | stdout | options:
2024-07-16 11:50:14 | INFO | stdout |   -h, --help            show this help message and exit
2024-07-16 11:50:14 | INFO | stdout |   --model_name_or_path MODEL_NAME_OR_PATH
2024-07-16 11:50:14 | INFO | stdout |                         Path to the model weight or identifier from huggingface.co/models or modelscope.cn/models. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --adapter_name_or_path ADAPTER_NAME_OR_PATH
2024-07-16 11:50:14 | INFO | stdout |                         Path to the adapter weight or identifier from huggingface.co/models. Use commas to separate multiple adapters. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --adapter_folder ADAPTER_FOLDER
2024-07-16 11:50:14 | INFO | stdout |                         The folder containing the adapter weights to load. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --cache_dir CACHE_DIR
2024-07-16 11:50:14 | INFO | stdout |                         Where to store the pre-trained models downloaded from huggingface.co or modelscope.cn. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --use_fast_tokenizer [USE_FAST_TOKENIZER]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_use_fast_tokenizer
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use one of the fast tokenizer (backed by the tokenizers library). (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --resize_vocab [RESIZE_VOCAB]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to resize the tokenizer vocab and the embedding layers. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --split_special_tokens [SPLIT_SPECIAL_TOKENS]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not the special tokens should be split during the tokenization process. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --new_special_tokens NEW_SPECIAL_TOKENS
2024-07-16 11:50:14 | INFO | stdout |                         Special tokens to be added into the tokenizer. Use commas to separate multiple tokens. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --model_revision MODEL_REVISION
2024-07-16 11:50:14 | INFO | stdout |                         The specific model version to use (can be a branch name, tag name or commit id). (default: main)
2024-07-16 11:50:14 | INFO | stdout |   --low_cpu_mem_usage [LOW_CPU_MEM_USAGE]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_low_cpu_mem_usage
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use memory-efficient model loading. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --quantization_method {bitsandbytes,hqq,eetq}
2024-07-16 11:50:14 | INFO | stdout |                         Quantization method to use for on-the-fly quantization. (default: bitsandbytes)
2024-07-16 11:50:14 | INFO | stdout |   --quantization_bit QUANTIZATION_BIT
2024-07-16 11:50:14 | INFO | stdout |                         The number of bits to quantize the model using bitsandbytes. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --quantization_type {fp4,nf4}
2024-07-16 11:50:14 | INFO | stdout |                         Quantization data type to use in int4 training. (default: nf4)
2024-07-16 11:50:14 | INFO | stdout |   --double_quantization [DOUBLE_QUANTIZATION]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_double_quantization
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use double quantization in int4 training. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --quantization_device_map {auto}
2024-07-16 11:50:14 | INFO | stdout |                         Device map used to infer the 4-bit quantized model, needs bitsandbytes>=0.43.0. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --rope_scaling {linear,dynamic}
2024-07-16 11:50:14 | INFO | stdout |                         Which scaling strategy should be adopted for the RoPE embeddings. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --flash_attn {auto,disabled,sdpa,fa2}
2024-07-16 11:50:14 | INFO | stdout |                         Enable FlashAttention for faster training and inference. (default: auto)
2024-07-16 11:50:14 | INFO | stdout |   --shift_attn [SHIFT_ATTN]
2024-07-16 11:50:14 | INFO | stdout |                         Enable shift short attention (S^2-Attn) proposed by LongLoRA. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --mixture_of_depths {convert,load}
2024-07-16 11:50:14 | INFO | stdout |                         Convert the model to mixture-of-depths (MoD) or load the MoD model. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --use_unsloth [USE_UNSLOTH]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use unsloth's optimization for the LoRA training. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --visual_inputs [VISUAL_INPUTS]
2024-07-16 11:50:14 | INFO | stdout |                         Whethor or not to use multimodal LLM that accepts visual inputs. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --moe_aux_loss_coef MOE_AUX_LOSS_COEF
2024-07-16 11:50:14 | INFO | stdout |                         Coefficient of the auxiliary router loss in mixture-of-experts model. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to disable gradient checkpointing. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --upcast_layernorm [UPCAST_LAYERNORM]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to upcast the layernorm weights in fp32. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to upcast the output of lm_head in fp32. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --train_from_scratch [TRAIN_FROM_SCRATCH]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to randomly initialize the model weights. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --infer_backend {huggingface,vllm}
2024-07-16 11:50:14 | INFO | stdout |                         Backend engine used at inference. (default: huggingface)
2024-07-16 11:50:14 | INFO | stdout |   --vllm_maxlen VLLM_MAXLEN
2024-07-16 11:50:14 | INFO | stdout |                         Maximum sequence (prompt + response) length of the vLLM engine. (default: 2048)
2024-07-16 11:50:14 | INFO | stdout |   --vllm_gpu_util VLLM_GPU_UTIL
2024-07-16 11:50:14 | INFO | stdout |                         The fraction of GPU memory in (0,1) to be used for the vLLM engine. (default: 0.9)
2024-07-16 11:50:14 | INFO | stdout |   --vllm_enforce_eager [VLLM_ENFORCE_EAGER]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to disable CUDA graph in the vLLM engine. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --vllm_max_lora_rank VLLM_MAX_LORA_RANK
2024-07-16 11:50:14 | INFO | stdout |                         Maximum rank of all LoRAs in the vLLM engine. (default: 32)
2024-07-16 11:50:14 | INFO | stdout |   --offload_folder OFFLOAD_FOLDER
2024-07-16 11:50:14 | INFO | stdout |                         Path to offload model weights. (default: offload)
2024-07-16 11:50:14 | INFO | stdout |   --use_cache [USE_CACHE]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use KV cache in generation. (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_use_cache        Whether or not to use KV cache in generation. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --infer_dtype {auto,float16,bfloat16,float32}
2024-07-16 11:50:14 | INFO | stdout |                         Data type for model weights and activations at inference. (default: auto)
2024-07-16 11:50:14 | INFO | stdout |   --hf_hub_token HF_HUB_TOKEN
2024-07-16 11:50:14 | INFO | stdout |                         Auth token to log in with Hugging Face Hub. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --ms_hub_token MS_HUB_TOKEN
2024-07-16 11:50:14 | INFO | stdout |                         Auth token to log in with ModelScope Hub. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --export_dir EXPORT_DIR
2024-07-16 11:50:14 | INFO | stdout |                         Path to the directory to save the exported model. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --export_size EXPORT_SIZE
2024-07-16 11:50:14 | INFO | stdout |                         The file shard size (in GB) of the exported model. (default: 1)
2024-07-16 11:50:14 | INFO | stdout |   --export_device {cpu,auto}
2024-07-16 11:50:14 | INFO | stdout |                         The device used in model export, use `auto` to accelerate exporting. (default: cpu)
2024-07-16 11:50:14 | INFO | stdout |   --export_quantization_bit EXPORT_QUANTIZATION_BIT
2024-07-16 11:50:14 | INFO | stdout |                         The number of bits to quantize the exported model. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --export_quantization_dataset EXPORT_QUANTIZATION_DATASET
2024-07-16 11:50:14 | INFO | stdout |                         Path to the dataset or dataset name to use in quantizing the exported model. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES
2024-07-16 11:50:14 | INFO | stdout |                         The number of samples used for quantization. (default: 128)
2024-07-16 11:50:14 | INFO | stdout |   --export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN
2024-07-16 11:50:14 | INFO | stdout |                         The maximum length of the model inputs used for quantization. (default: 1024)
2024-07-16 11:50:14 | INFO | stdout |   --export_legacy_format [EXPORT_LEGACY_FORMAT]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to save the `.bin` files instead of `.safetensors`. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --export_hub_model_id EXPORT_HUB_MODEL_ID
2024-07-16 11:50:14 | INFO | stdout |                         The name of the repository if push the model to the Hugging Face hub. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --print_param_status [PRINT_PARAM_STATUS]
2024-07-16 11:50:14 | INFO | stdout |                         For debugging purposes, print the status of the parameters in the model. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --template TEMPLATE   Which template to use for constructing prompts in training and inference. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --dataset DATASET     The name of provided dataset(s) to use. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --dataset_dir DATASET_DIR
2024-07-16 11:50:14 | INFO | stdout |                         Path to the folder containing the datasets. (default: data)
2024-07-16 11:50:14 | INFO | stdout |   --split SPLIT         Which dataset split to use for training and evaluation. (default: train)
2024-07-16 11:50:14 | INFO | stdout |   --cutoff_len CUTOFF_LEN
2024-07-16 11:50:14 | INFO | stdout |                         The cutoff length of the tokenized inputs in the dataset. (default: 4096)
2024-07-16 11:50:14 | INFO | stdout |   --reserved_label_len RESERVED_LABEL_LEN
2024-07-16 11:50:14 | INFO | stdout |                         The minimum cutoff length reserved for the tokenized labels in the dataset. (default: 1)
2024-07-16 11:50:14 | INFO | stdout |   --train_on_prompt [TRAIN_ON_PROMPT]
2024-07-16 11:50:14 | INFO | stdout |                         Whether to disable the mask on the prompt or not. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --streaming [STREAMING]
2024-07-16 11:50:14 | INFO | stdout |                         Enable dataset streaming. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --buffer_size BUFFER_SIZE
2024-07-16 11:50:14 | INFO | stdout |                         Size of the buffer to randomly sample examples from in dataset streaming. (default: 16384)
2024-07-16 11:50:14 | INFO | stdout |   --mix_strategy {concat,interleave_under,interleave_over}
2024-07-16 11:50:14 | INFO | stdout |                         Strategy to use in dataset mixing (concat/interleave) (undersampling/oversampling). (default: concat)
2024-07-16 11:50:14 | INFO | stdout |   --interleave_probs INTERLEAVE_PROBS
2024-07-16 11:50:14 | INFO | stdout |                         Probabilities to sample data from datasets. Use commas to separate multiple datasets. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --overwrite_cache [OVERWRITE_CACHE]
2024-07-16 11:50:14 | INFO | stdout |                         Overwrite the cached training and evaluation sets. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --preprocessing_num_workers PREPROCESSING_NUM_WORKERS
2024-07-16 11:50:14 | INFO | stdout |                         The number of processes to use for the pre-processing. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --max_samples MAX_SAMPLES
2024-07-16 11:50:14 | INFO | stdout |                         For debugging purposes, truncate the number of examples for each dataset. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --eval_num_beams EVAL_NUM_BEAMS
2024-07-16 11:50:14 | INFO | stdout |                         Number of beams to use for evaluation. This argument will be passed to `model.generate` (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_ignore_pad_token_for_loss
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to ignore the tokens corresponding to padded labels in the loss computation. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --val_size VAL_SIZE   Size of the development set, should be an integer or a float in range `[0,1)`. (default: 0.0)
2024-07-16 11:50:14 | INFO | stdout |   --packing PACKING     Whether or not to pack the sequences in training. Will automatically enable in pre-training. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --tool_format TOOL_FORMAT
2024-07-16 11:50:14 | INFO | stdout |                         Tool format to use for constructing function calling examples. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --tokenized_path TOKENIZED_PATH
2024-07-16 11:50:14 | INFO | stdout |                         Path to save or load the tokenized datasets. (default: None)
2024-07-16 11:50:14 | INFO | stdout |   --do_sample [DO_SAMPLE]
2024-07-16 11:50:14 | INFO | stdout |                         Whether or not to use sampling, use greedy decoding otherwise. (default: True)
2024-07-16 11:50:14 | INFO | stdout |   --no_do_sample        Whether or not to use sampling, use greedy decoding otherwise. (default: False)
2024-07-16 11:50:14 | INFO | stdout |   --temperature TEMPERATURE
2024-07-16 11:50:14 | INFO | stdout |                         The value used to modulate the next token probabilities. (default: 0.95)
2024-07-16 11:50:14 | INFO | stdout |   --top_p TOP_P         The smallest set of most probable tokens with probabilities that add up to top_p or higher are kept. (default: 0.7)
2024-07-16 11:50:14 | INFO | stdout |   --top_k TOP_K         The number of highest probability vocabulary tokens to keep for top-k filtering. (default: 50)
2024-07-16 11:50:14 | INFO | stdout |   --num_beams NUM_BEAMS
2024-07-16 11:50:14 | INFO | stdout |                         Number of beams for beam search. 1 means no beam search. (default: 1)
2024-07-16 11:50:14 | INFO | stdout |   --max_length MAX_LENGTH
2024-07-16 11:50:14 | INFO | stdout |                         The maximum length the generated tokens can have. It can be overridden by max_new_tokens. (default: 1024)
2024-07-16 11:50:14 | INFO | stdout |   --max_new_tokens MAX_NEW_TOKENS
2024-07-16 11:50:14 | INFO | stdout |                         The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt. (default: 1024)
2024-07-16 11:50:14 | INFO | stdout |   --repetition_penalty REPETITION_PENALTY
2024-07-16 11:50:14 | INFO | stdout |                         The parameter for repetition penalty. 1.0 means no penalty. (default: 1.0)
2024-07-16 11:50:14 | INFO | stdout |   --length_penalty LENGTH_PENALTY
2024-07-16 11:50:14 | INFO | stdout |                         Exponential penalty to the length that is used with beam-based generation. (default: 1.0)
2024-07-16 11:50:14 | INFO | stdout |   --default_system DEFAULT_SYSTEM
2024-07-16 11:50:14 | INFO | stdout |                         Default system message to use in chat completion. (default: None)
2024-07-16 11:50:14 | INFO | stdout | 
2024-07-16 11:50:14 | INFO | stdout | Got unknown args, potentially deprecated arguments: ['--n_shot', '5']
2024-07-16 11:50:14 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:50:14 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:50:14 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:50:14 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:50:14 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:50:14 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 141, in _parse_infer_args
2024-07-16 11:50:14 | ERROR | stderr |     return _parse_args(parser, args)
2024-07-16 11:50:14 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 73, in _parse_args
2024-07-16 11:50:14 | ERROR | stderr |     raise ValueError("Some specified arguments are not used by the HfArgumentParser: {}".format(unknown_args))
2024-07-16 11:50:14 | ERROR | stderr | ValueError: Some specified arguments are not used by the HfArgumentParser: ['--n_shot', '5']
2024-07-16 11:50:34 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:50:34 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:50:34 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:50:34 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 335, in get_infer_args
2024-07-16 11:50:34 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:50:34 | ERROR | stderr | ValueError: not enough values to unpack (expected 4, got 3)
2024-07-16 11:54:09 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 11:54:09 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 336, in get_infer_args
2024-07-16 11:54:09 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = _parse_infer_args(args)
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 141, in _parse_infer_args
2024-07-16 11:54:09 | ERROR | stderr |     parser = HfArgumentParser(_INFER_ARGS)
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/hf_argparser.py", line 137, in __init__
2024-07-16 11:54:09 | ERROR | stderr |     self._add_dataclass_arguments(dtype)
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/hf_argparser.py", line 260, in _add_dataclass_arguments
2024-07-16 11:54:09 | ERROR | stderr |     for field in dataclasses.fields(dtype):
2024-07-16 11:54:09 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/dataclasses.py", line 1198, in fields
2024-07-16 11:54:09 | ERROR | stderr |     raise TypeError('must be called with a dataclass type or instance') from None
2024-07-16 11:54:09 | ERROR | stderr | TypeError: must be called with a dataclass type or instance
2024-07-16 14:01:31 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 14:01:31 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 14:01:31 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 14:01:31 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 344, in get_infer_args
2024-07-16 14:01:31 | ERROR | stderr |     if finetuning_args.stage != "sft":
2024-07-16 14:01:31 | ERROR | stderr | AttributeError: 'RetrivModelSFTArguments' object has no attribute 'stage'
2024-07-16 14:20:11 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 14:20:11 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 14:20:11 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 14:20:11 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 344, in get_infer_args
2024-07-16 14:20:11 | ERROR | stderr |     if finetuning_args.stage != "sft":
2024-07-16 14:20:11 | ERROR | stderr | AttributeError: 'RetrivModelSFTArguments' object has no attribute 'stage'
2024-07-16 15:59:38 | ERROR | stderr | Traceback (most recent call last):
2024-07-16 15:59:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/test/tmp.py", line 23, in <module>
2024-07-16 15:59:38 | ERROR | stderr |     model_args, data_args, finetuning_args, generating_args = get_infer_args()
2024-07-16 15:59:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 357, in get_infer_args
2024-07-16 15:59:38 | ERROR | stderr |     _check_extra_dependencies(model_args, finetuning_args)
2024-07-16 15:59:38 | ERROR | stderr |   File "/home/zhangyh/projs/rrag/rrag/argument/parser.py", line 115, in _check_extra_dependencies
2024-07-16 15:59:38 | ERROR | stderr |     require_version("vllm>=0.4.3", "To fix: pip install vllm>=0.4.3")
2024-07-16 15:59:38 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 111, in require_version
2024-07-16 15:59:38 | ERROR | stderr |     _compare_versions(op, got_ver, want_ver, requirement, pkg, hint)
2024-07-16 15:59:38 | ERROR | stderr |   File "/home/zhangyh/miniconda3/envs/rag_nlp_rl/lib/python3.10/site-packages/transformers/utils/versions.py", line 44, in _compare_versions
2024-07-16 15:59:38 | ERROR | stderr |     raise ImportError(
2024-07-16 15:59:38 | ERROR | stderr | ImportError: vllm>=0.4.3 is required for a normal functioning of this module, but found vllm==0.3.3.
2024-07-16 15:59:38 | ERROR | stderr | To fix: pip install vllm>=0.4.3
2024-07-16 16:02:23 | INFO | stdout | ModelArguments(model_name_or_path='/home/zhangyh/projs/LLaMA-Factory/output_qwen2_0715_e1', adapter_name_or_path=None, adapter_folder=None, cache_dir=None, use_fast_tokenizer=True, resize_vocab=False, split_special_tokens=False, new_special_tokens=None, model_revision='main', low_cpu_mem_usage=True, quantization_method='bitsandbytes', quantization_bit=None, quantization_type='nf4', double_quantization=True, quantization_device_map=None, rope_scaling=None, flash_attn='auto', shift_attn=False, mixture_of_depths=None, use_unsloth=False, visual_inputs=False, moe_aux_loss_coef=None, disable_gradient_checkpointing=False, upcast_layernorm=False, upcast_lmhead_output=False, train_from_scratch=False, infer_backend='vllm', vllm_maxlen=2048, vllm_gpu_util=0.9, vllm_enforce_eager=False, vllm_max_lora_rank=32, offload_folder='offload', use_cache=True, infer_dtype='auto', hf_hub_token=None, ms_hub_token=None, export_dir=None, export_size=1, export_device='cpu', export_quantization_bit=None, export_quantization_dataset=None, export_quantization_nsamples=128, export_quantization_maxlen=1024, export_legacy_format=False, export_hub_model_id=None, print_param_status=False)
2024-07-16 16:12:44 | INFO | stdout | ModelArguments(model_name_or_path='/home/zhangyh/projs/LLaMA-Factory/output_qwen2_0715_e1', adapter_name_or_path=None, adapter_folder=None, cache_dir=None, use_fast_tokenizer=True, resize_vocab=False, split_special_tokens=False, new_special_tokens=None, model_revision='main', low_cpu_mem_usage=True, quantization_method='bitsandbytes', quantization_bit=None, quantization_type='nf4', double_quantization=True, quantization_device_map=None, rope_scaling=None, flash_attn='auto', shift_attn=False, mixture_of_depths=None, use_unsloth=False, visual_inputs=False, moe_aux_loss_coef=None, disable_gradient_checkpointing=False, upcast_layernorm=False, upcast_lmhead_output=False, train_from_scratch=False, infer_backend='vllm', vllm_maxlen=2048, vllm_gpu_util=0.9, vllm_enforce_eager=False, vllm_max_lora_rank=32, offload_folder='offload', use_cache=True, infer_dtype='auto', hf_hub_token=None, ms_hub_token=None, export_dir=None, export_size=1, export_device='cpu', export_quantization_bit=None, export_quantization_dataset=None, export_quantization_nsamples=128, export_quantization_maxlen=1024, export_legacy_format=False, export_hub_model_id=None, print_param_status=False)
2024-07-16 16:52:16 | ERROR | stderr | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
2024-07-16 16:52:16 | ERROR | stderr |               [--adapter_name_or_path ADAPTER_NAME_OR_PATH]
2024-07-16 16:52:16 | ERROR | stderr |               [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR]
2024-07-16 16:52:16 | ERROR | stderr |               [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]]
2024-07-16 16:52:16 | ERROR | stderr |               [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]
2024-07-16 16:52:16 | ERROR | stderr |               [--new_special_tokens NEW_SPECIAL_TOKENS]
2024-07-16 16:52:16 | ERROR | stderr |               [--model_revision MODEL_REVISION]
2024-07-16 16:52:16 | ERROR | stderr |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_low_cpu_mem_usage]
2024-07-16 16:52:16 | ERROR | stderr |               [--quantization_method {bitsandbytes,hqq,eetq}]
2024-07-16 16:52:16 | ERROR | stderr |               [--quantization_bit QUANTIZATION_BIT]
2024-07-16 16:52:16 | ERROR | stderr |               [--quantization_type {fp4,nf4}]
2024-07-16 16:52:16 | ERROR | stderr |               [--double_quantization [DOUBLE_QUANTIZATION]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_double_quantization] [--quantization_device_map {auto}]
2024-07-16 16:52:16 | ERROR | stderr |               [--rope_scaling {linear,dynamic}]
2024-07-16 16:52:16 | ERROR | stderr |               [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 16:52:16 | ERROR | stderr |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}]
2024-07-16 16:52:16 | ERROR | stderr |               [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]]
2024-07-16 16:52:16 | ERROR | stderr |               [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 16:52:16 | ERROR | stderr |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]
2024-07-16 16:52:16 | ERROR | stderr |               [--upcast_layernorm [UPCAST_LAYERNORM]]
2024-07-16 16:52:16 | ERROR | stderr |               [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]
2024-07-16 16:52:16 | ERROR | stderr |               [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 16:52:16 | ERROR | stderr |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN]
2024-07-16 16:52:16 | ERROR | stderr |               [--vllm_gpu_util VLLM_GPU_UTIL]
2024-07-16 16:52:16 | ERROR | stderr |               [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]
2024-07-16 16:52:16 | ERROR | stderr |               [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 16:52:16 | ERROR | stderr |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}]
2024-07-16 16:52:16 | ERROR | stderr |               [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_device {cpu,auto}]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_quantization_bit EXPORT_QUANTIZATION_BIT]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 16:52:16 | ERROR | stderr |               [--export_hub_model_id EXPORT_HUB_MODEL_ID]
2024-07-16 16:52:16 | ERROR | stderr |               [--print_param_status [PRINT_PARAM_STATUS]]
2024-07-16 16:52:16 | ERROR | stderr |               [--template TEMPLATE] [--dataset DATASET]
2024-07-16 16:52:16 | ERROR | stderr |               [--dataset_dir DATASET_DIR] [--split SPLIT]
2024-07-16 16:52:16 | ERROR | stderr |               [--cutoff_len CUTOFF_LEN]
2024-07-16 16:52:16 | ERROR | stderr |               [--reserved_label_len RESERVED_LABEL_LEN]
2024-07-16 16:52:16 | ERROR | stderr |               [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]]
2024-07-16 16:52:16 | ERROR | stderr |               [--buffer_size BUFFER_SIZE]
2024-07-16 16:52:16 | ERROR | stderr |               [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 16:52:16 | ERROR | stderr |               [--interleave_probs INTERLEAVE_PROBS]
2024-07-16 16:52:16 | ERROR | stderr |               [--overwrite_cache [OVERWRITE_CACHE]]
2024-07-16 16:52:16 | ERROR | stderr |               [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
2024-07-16 16:52:16 | ERROR | stderr |               [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 16:52:16 | ERROR | stderr |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]
2024-07-16 16:52:16 | ERROR | stderr |               [--packing PACKING] [--tool_format TOOL_FORMAT]
2024-07-16 16:52:16 | ERROR | stderr |               [--tokenized_path TOKENIZED_PATH]
2024-07-16 16:52:16 | ERROR | stderr |               [--freeze_trainable_layers_ss FREEZE_TRAINABLE_LAYERS_SS]
2024-07-16 16:52:16 | ERROR | stderr |               [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_start_block BADAM_START_BLOCK]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_switch_mode {ascending,descending,random,fixed}]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_switch_interval BADAM_SWITCH_INTERVAL]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_update_ratio BADAM_UPDATE_RATIO]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_mask_mode {adjacent,scatter}]
2024-07-16 16:52:16 | ERROR | stderr |               [--badam_verbose BADAM_VERBOSE] [--use_galore [USE_GALORE]]
2024-07-16 16:52:16 | ERROR | stderr |               [--galore_target GALORE_TARGET] [--galore_rank GALORE_RANK]
2024-07-16 16:52:16 | ERROR | stderr |               [--galore_update_interval GALORE_UPDATE_INTERVAL]
2024-07-16 16:52:16 | ERROR | stderr |               [--galore_scale GALORE_SCALE]
2024-07-16 16:52:16 | ERROR | stderr |               [--galore_proj_type {std,reverse_std,right,left,full}]
2024-07-16 16:52:16 | ERROR | stderr |               [--galore_layerwise [GALORE_LAYERWISE]] [--pref_beta PREF_BETA]
2024-07-16 16:52:16 | ERROR | stderr |               [--pref_ftx PREF_FTX]
2024-07-16 16:52:16 | ERROR | stderr |               [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]
2024-07-16 16:52:16 | ERROR | stderr |               [--dpo_label_smoothing DPO_LABEL_SMOOTHING]
2024-07-16 16:52:16 | ERROR | stderr |               [--kto_chosen_weight KTO_CHOSEN_WEIGHT]
2024-07-16 16:52:16 | ERROR | stderr |               [--kto_rejected_weight KTO_REJECTED_WEIGHT]
2024-07-16 16:52:16 | ERROR | stderr |               [--simpo_gamma SIMPO_GAMMA] [--ppo_buffer_size PPO_BUFFER_SIZE]
2024-07-16 16:52:16 | ERROR | stderr |               [--ppo_epochs PPO_EPOCHS] [--ppo_score_norm [PPO_SCORE_NORM]]
2024-07-16 16:52:16 | ERROR | stderr |               [--ppo_target PPO_TARGET]
2024-07-16 16:52:16 | ERROR | stderr |               [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]
2024-07-16 16:52:16 | ERROR | stderr |               [--ref_model REF_MODEL]
2024-07-16 16:52:16 | ERROR | stderr |               [--ref_model_adapters REF_MODEL_ADAPTERS]
2024-07-16 16:52:16 | ERROR | stderr |               [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]
2024-07-16 16:52:16 | ERROR | stderr |               [--reward_model REWARD_MODEL]
2024-07-16 16:52:16 | ERROR | stderr |               [--reward_model_adapters REWARD_MODEL_ADAPTERS]
2024-07-16 16:52:16 | ERROR | stderr |               [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]
2024-07-16 16:52:16 | ERROR | stderr |               [--reward_model_type {lora,full,api}]
2024-07-16 16:52:16 | ERROR | stderr |               [--additional_target ADDITIONAL_TARGET]
2024-07-16 16:52:16 | ERROR | stderr |               [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]
2024-07-16 16:52:16 | ERROR | stderr |               [--lora_rank LORA_RANK] [--lora_target LORA_TARGET]
2024-07-16 16:52:16 | ERROR | stderr |               [--loraplus_lr_ratio LORAPLUS_LR_RATIO]
2024-07-16 16:52:16 | ERROR | stderr |               [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]
2024-07-16 16:52:16 | ERROR | stderr |               [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]
2024-07-16 16:52:16 | ERROR | stderr |               [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]
2024-07-16 16:52:16 | ERROR | stderr |               [--pissa_convert [PISSA_CONVERT]]
2024-07-16 16:52:16 | ERROR | stderr |               [--create_new_adapter [CREATE_NEW_ADAPTER]]
2024-07-16 16:52:16 | ERROR | stderr |               [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]
2024-07-16 16:52:16 | ERROR | stderr |               [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]
2024-07-16 16:52:16 | ERROR | stderr |               [--freeze_extra_modules FREEZE_EXTRA_MODULES]
2024-07-16 16:52:16 | ERROR | stderr |               [--pure_bf16 [PURE_BF16]] [--stage {pt,sft,rm,ppo,dpo,kto}]
2024-07-16 16:52:16 | ERROR | stderr |               [--finetuning_type {lora,freeze,full}]
2024-07-16 16:52:16 | ERROR | stderr |               [--use_llama_pro [USE_LLAMA_PRO]]
2024-07-16 16:52:16 | ERROR | stderr |               [--freeze_vision_tower [FREEZE_VISION_TOWER]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_freeze_vision_tower]
2024-07-16 16:52:16 | ERROR | stderr |               [--train_mm_proj_only [TRAIN_MM_PROJ_ONLY]]
2024-07-16 16:52:16 | ERROR | stderr |               [--plot_loss [PLOT_LOSS]] [--do_sample [DO_SAMPLE]]
2024-07-16 16:52:16 | ERROR | stderr |               [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P]
2024-07-16 16:52:16 | ERROR | stderr |               [--top_k TOP_K] [--num_beams NUM_BEAMS]
2024-07-16 16:52:16 | ERROR | stderr |               [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 16:52:16 | ERROR | stderr |               [--repetition_penalty REPETITION_PENALTY]
2024-07-16 16:52:16 | ERROR | stderr |               [--length_penalty LENGTH_PENALTY]
2024-07-16 16:52:16 | ERROR | stderr |               [--default_system DEFAULT_SYSTEM]
2024-07-16 16:52:16 | ERROR | stderr | tmp.py: error: the following arguments are required: --model_name_or_path
2024-07-16 16:53:54 | ERROR | stderr | usage: tmp.py [-h] --model_name_or_path MODEL_NAME_OR_PATH
2024-07-16 16:53:54 | ERROR | stderr |               [--adapter_name_or_path ADAPTER_NAME_OR_PATH]
2024-07-16 16:53:54 | ERROR | stderr |               [--adapter_folder ADAPTER_FOLDER] [--cache_dir CACHE_DIR]
2024-07-16 16:53:54 | ERROR | stderr |               [--use_fast_tokenizer [USE_FAST_TOKENIZER]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_use_fast_tokenizer] [--resize_vocab [RESIZE_VOCAB]]
2024-07-16 16:53:54 | ERROR | stderr |               [--split_special_tokens [SPLIT_SPECIAL_TOKENS]]
2024-07-16 16:53:54 | ERROR | stderr |               [--new_special_tokens NEW_SPECIAL_TOKENS]
2024-07-16 16:53:54 | ERROR | stderr |               [--model_revision MODEL_REVISION]
2024-07-16 16:53:54 | ERROR | stderr |               [--low_cpu_mem_usage [LOW_CPU_MEM_USAGE]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_low_cpu_mem_usage]
2024-07-16 16:53:54 | ERROR | stderr |               [--quantization_method {bitsandbytes,hqq,eetq}]
2024-07-16 16:53:54 | ERROR | stderr |               [--quantization_bit QUANTIZATION_BIT]
2024-07-16 16:53:54 | ERROR | stderr |               [--quantization_type {fp4,nf4}]
2024-07-16 16:53:54 | ERROR | stderr |               [--double_quantization [DOUBLE_QUANTIZATION]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_double_quantization] [--quantization_device_map {auto}]
2024-07-16 16:53:54 | ERROR | stderr |               [--rope_scaling {linear,dynamic}]
2024-07-16 16:53:54 | ERROR | stderr |               [--flash_attn {auto,disabled,sdpa,fa2}]
2024-07-16 16:53:54 | ERROR | stderr |               [--shift_attn [SHIFT_ATTN]] [--mixture_of_depths {convert,load}]
2024-07-16 16:53:54 | ERROR | stderr |               [--use_unsloth [USE_UNSLOTH]] [--visual_inputs [VISUAL_INPUTS]]
2024-07-16 16:53:54 | ERROR | stderr |               [--moe_aux_loss_coef MOE_AUX_LOSS_COEF]
2024-07-16 16:53:54 | ERROR | stderr |               [--disable_gradient_checkpointing [DISABLE_GRADIENT_CHECKPOINTING]]
2024-07-16 16:53:54 | ERROR | stderr |               [--upcast_layernorm [UPCAST_LAYERNORM]]
2024-07-16 16:53:54 | ERROR | stderr |               [--upcast_lmhead_output [UPCAST_LMHEAD_OUTPUT]]
2024-07-16 16:53:54 | ERROR | stderr |               [--train_from_scratch [TRAIN_FROM_SCRATCH]]
2024-07-16 16:53:54 | ERROR | stderr |               [--infer_backend {huggingface,vllm}] [--vllm_maxlen VLLM_MAXLEN]
2024-07-16 16:53:54 | ERROR | stderr |               [--vllm_gpu_util VLLM_GPU_UTIL]
2024-07-16 16:53:54 | ERROR | stderr |               [--vllm_enforce_eager [VLLM_ENFORCE_EAGER]]
2024-07-16 16:53:54 | ERROR | stderr |               [--vllm_max_lora_rank VLLM_MAX_LORA_RANK]
2024-07-16 16:53:54 | ERROR | stderr |               [--offload_folder OFFLOAD_FOLDER] [--use_cache [USE_CACHE]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_use_cache] [--infer_dtype {auto,float16,bfloat16,float32}]
2024-07-16 16:53:54 | ERROR | stderr |               [--hf_hub_token HF_HUB_TOKEN] [--ms_hub_token MS_HUB_TOKEN]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_dir EXPORT_DIR] [--export_size EXPORT_SIZE]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_device {cpu,auto}]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_quantization_bit EXPORT_QUANTIZATION_BIT]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_quantization_dataset EXPORT_QUANTIZATION_DATASET]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_quantization_nsamples EXPORT_QUANTIZATION_NSAMPLES]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_quantization_maxlen EXPORT_QUANTIZATION_MAXLEN]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_legacy_format [EXPORT_LEGACY_FORMAT]]
2024-07-16 16:53:54 | ERROR | stderr |               [--export_hub_model_id EXPORT_HUB_MODEL_ID]
2024-07-16 16:53:54 | ERROR | stderr |               [--print_param_status [PRINT_PARAM_STATUS]]
2024-07-16 16:53:54 | ERROR | stderr |               [--template TEMPLATE] [--dataset DATASET]
2024-07-16 16:53:54 | ERROR | stderr |               [--dataset_dir DATASET_DIR] [--split SPLIT]
2024-07-16 16:53:54 | ERROR | stderr |               [--cutoff_len CUTOFF_LEN]
2024-07-16 16:53:54 | ERROR | stderr |               [--reserved_label_len RESERVED_LABEL_LEN]
2024-07-16 16:53:54 | ERROR | stderr |               [--train_on_prompt [TRAIN_ON_PROMPT]] [--streaming [STREAMING]]
2024-07-16 16:53:54 | ERROR | stderr |               [--buffer_size BUFFER_SIZE]
2024-07-16 16:53:54 | ERROR | stderr |               [--mix_strategy {concat,interleave_under,interleave_over}]
2024-07-16 16:53:54 | ERROR | stderr |               [--interleave_probs INTERLEAVE_PROBS]
2024-07-16 16:53:54 | ERROR | stderr |               [--overwrite_cache [OVERWRITE_CACHE]]
2024-07-16 16:53:54 | ERROR | stderr |               [--preprocessing_num_workers PREPROCESSING_NUM_WORKERS]
2024-07-16 16:53:54 | ERROR | stderr |               [--max_samples MAX_SAMPLES] [--eval_num_beams EVAL_NUM_BEAMS]
2024-07-16 16:53:54 | ERROR | stderr |               [--ignore_pad_token_for_loss [IGNORE_PAD_TOKEN_FOR_LOSS]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_ignore_pad_token_for_loss] [--val_size VAL_SIZE]
2024-07-16 16:53:54 | ERROR | stderr |               [--packing PACKING] [--tool_format TOOL_FORMAT]
2024-07-16 16:53:54 | ERROR | stderr |               [--tokenized_path TOKENIZED_PATH]
2024-07-16 16:53:54 | ERROR | stderr |               [--freeze_trainable_layers_ss FREEZE_TRAINABLE_LAYERS_SS]
2024-07-16 16:53:54 | ERROR | stderr |               [--use_badam [USE_BADAM]] [--badam_mode {layer,ratio}]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_start_block BADAM_START_BLOCK]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_switch_mode {ascending,descending,random,fixed}]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_switch_interval BADAM_SWITCH_INTERVAL]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_update_ratio BADAM_UPDATE_RATIO]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_mask_mode {adjacent,scatter}]
2024-07-16 16:53:54 | ERROR | stderr |               [--badam_verbose BADAM_VERBOSE] [--use_galore [USE_GALORE]]
2024-07-16 16:53:54 | ERROR | stderr |               [--galore_target GALORE_TARGET] [--galore_rank GALORE_RANK]
2024-07-16 16:53:54 | ERROR | stderr |               [--galore_update_interval GALORE_UPDATE_INTERVAL]
2024-07-16 16:53:54 | ERROR | stderr |               [--galore_scale GALORE_SCALE]
2024-07-16 16:53:54 | ERROR | stderr |               [--galore_proj_type {std,reverse_std,right,left,full}]
2024-07-16 16:53:54 | ERROR | stderr |               [--galore_layerwise [GALORE_LAYERWISE]] [--pref_beta PREF_BETA]
2024-07-16 16:53:54 | ERROR | stderr |               [--pref_ftx PREF_FTX]
2024-07-16 16:53:54 | ERROR | stderr |               [--pref_loss {sigmoid,hinge,ipo,kto_pair,orpo,simpo}]
2024-07-16 16:53:54 | ERROR | stderr |               [--dpo_label_smoothing DPO_LABEL_SMOOTHING]
2024-07-16 16:53:54 | ERROR | stderr |               [--kto_chosen_weight KTO_CHOSEN_WEIGHT]
2024-07-16 16:53:54 | ERROR | stderr |               [--kto_rejected_weight KTO_REJECTED_WEIGHT]
2024-07-16 16:53:54 | ERROR | stderr |               [--simpo_gamma SIMPO_GAMMA] [--ppo_buffer_size PPO_BUFFER_SIZE]
2024-07-16 16:53:54 | ERROR | stderr |               [--ppo_epochs PPO_EPOCHS] [--ppo_score_norm [PPO_SCORE_NORM]]
2024-07-16 16:53:54 | ERROR | stderr |               [--ppo_target PPO_TARGET]
2024-07-16 16:53:54 | ERROR | stderr |               [--ppo_whiten_rewards [PPO_WHITEN_REWARDS]]
2024-07-16 16:53:54 | ERROR | stderr |               [--ref_model REF_MODEL]
2024-07-16 16:53:54 | ERROR | stderr |               [--ref_model_adapters REF_MODEL_ADAPTERS]
2024-07-16 16:53:54 | ERROR | stderr |               [--ref_model_quantization_bit REF_MODEL_QUANTIZATION_BIT]
2024-07-16 16:53:54 | ERROR | stderr |               [--reward_model REWARD_MODEL]
2024-07-16 16:53:54 | ERROR | stderr |               [--reward_model_adapters REWARD_MODEL_ADAPTERS]
2024-07-16 16:53:54 | ERROR | stderr |               [--reward_model_quantization_bit REWARD_MODEL_QUANTIZATION_BIT]
2024-07-16 16:53:54 | ERROR | stderr |               [--reward_model_type {lora,full,api}]
2024-07-16 16:53:54 | ERROR | stderr |               [--additional_target ADDITIONAL_TARGET]
2024-07-16 16:53:54 | ERROR | stderr |               [--lora_alpha LORA_ALPHA] [--lora_dropout LORA_DROPOUT]
2024-07-16 16:53:54 | ERROR | stderr |               [--lora_rank LORA_RANK] [--lora_target LORA_TARGET]
2024-07-16 16:53:54 | ERROR | stderr |               [--loraplus_lr_ratio LORAPLUS_LR_RATIO]
2024-07-16 16:53:54 | ERROR | stderr |               [--loraplus_lr_embedding LORAPLUS_LR_EMBEDDING]
2024-07-16 16:53:54 | ERROR | stderr |               [--use_rslora [USE_RSLORA]] [--use_dora [USE_DORA]]
2024-07-16 16:53:54 | ERROR | stderr |               [--pissa_init [PISSA_INIT]] [--pissa_iter PISSA_ITER]
2024-07-16 16:53:54 | ERROR | stderr |               [--pissa_convert [PISSA_CONVERT]]
2024-07-16 16:53:54 | ERROR | stderr |               [--create_new_adapter [CREATE_NEW_ADAPTER]]
2024-07-16 16:53:54 | ERROR | stderr |               [--freeze_trainable_layers FREEZE_TRAINABLE_LAYERS]
2024-07-16 16:53:54 | ERROR | stderr |               [--freeze_trainable_modules FREEZE_TRAINABLE_MODULES]
2024-07-16 16:53:54 | ERROR | stderr |               [--freeze_extra_modules FREEZE_EXTRA_MODULES]
2024-07-16 16:53:54 | ERROR | stderr |               [--pure_bf16 [PURE_BF16]] [--stage {pt,sft,rm,ppo,dpo,kto}]
2024-07-16 16:53:54 | ERROR | stderr |               [--finetuning_type {lora,freeze,full}]
2024-07-16 16:53:54 | ERROR | stderr |               [--use_llama_pro [USE_LLAMA_PRO]]
2024-07-16 16:53:54 | ERROR | stderr |               [--freeze_vision_tower [FREEZE_VISION_TOWER]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_freeze_vision_tower]
2024-07-16 16:53:54 | ERROR | stderr |               [--train_mm_proj_only [TRAIN_MM_PROJ_ONLY]]
2024-07-16 16:53:54 | ERROR | stderr |               [--plot_loss [PLOT_LOSS]] [--do_sample [DO_SAMPLE]]
2024-07-16 16:53:54 | ERROR | stderr |               [--no_do_sample] [--temperature TEMPERATURE] [--top_p TOP_P]
2024-07-16 16:53:54 | ERROR | stderr |               [--top_k TOP_K] [--num_beams NUM_BEAMS]
2024-07-16 16:53:54 | ERROR | stderr |               [--max_length MAX_LENGTH] [--max_new_tokens MAX_NEW_TOKENS]
2024-07-16 16:53:54 | ERROR | stderr |               [--repetition_penalty REPETITION_PENALTY]
2024-07-16 16:53:54 | ERROR | stderr |               [--length_penalty LENGTH_PENALTY]
2024-07-16 16:53:54 | ERROR | stderr |               [--default_system DEFAULT_SYSTEM]
2024-07-16 16:53:54 | ERROR | stderr | tmp.py: error: the following arguments are required: --model_name_or_path
